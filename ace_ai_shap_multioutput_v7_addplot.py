"""
v2: use shap.kmeans from train set
v3: sum up SHAP values for similar features
v4: sort SHAP values based on value instead of ABS val, 
    input train_df (after kmeans) instead of X_train, 
    change output order to 'YEARH_', 'YEAR1_', 'YEAR1H_'
v5: modify functions for sequential model structure,
    load features_dict into function,
    parse in data not scaled,
    add random seed to explainer
v6: for shap_explainer multiple, take abs first before average
add periods to YEAR3
v7: remove unnecessary libraries for data robot custom model deployment

"""

# Imports & Declarations
import numpy as np
import pandas as pd
import shap
from tensorflow import keras
from pickle import load
import matplotlib.pyplot as plt

random_state = 1234
np.random.seed(random_state)

class DeepPatientShap:
    """

    Methods
    -------
    shap_explainer_generator: used to fit a KernelExplainer for a specified row to generate feature importances
    shap_top_features: list of top features generated from shap_explainer_generator
    shap_plot_features: plot of top features generated from shap_explainer_generator
    display_probabilities: list of probabilities of targets generated by keras NN model for specified row in shap_explainer_generator 

    Inputs
    -----
    scaler: the Scaler (standardscaler / minmaxscaler) used for preprocessing of data - used to transform dataset
    model: the keras NN model used 
    X_train_kmeans: the summarized dataset using kmeans
    X_test: numpy array before scaling - dataset to generate feature importance for 
    features: list of feature names used for training
    targets: list of targets for prediction
    ex_features: features that are non-periodic
    categorical_features: features dict to map root feature to dummies
    features_dict: periodic features dict to map root feature to periodic features

    """
    
    def __init__(self, scaler, model, X_train_kmeans, X_test, features, features_dict, targets, full_targets, categorical_features, ex_features):
    
        self.model = model
        self.X_test = X_test
        self.features = features
        self.targets = targets
        self.full_targets = full_targets
        self.scaler = scaler
        self.categorical_features = categorical_features       
        self.ex_features = ex_features
        self.train_df = X_train_kmeans   # use kmeans summarized data            
        self.time_features = features_dict
        self.X_scaled = scaler.transform(X_test)    
    
    # create function to generate shap explainer
    def shap_explainer_generator(self, row_ind, sampling_times=10000):
        
        # train_df is summarised X_train using kmeans
        shap_explainer = shap.KernelExplainer(self.model, self.train_df, seed=random_state, gpu_model=True)
        
        self.row_ind = row_ind
        
        shap_value_single = shap_explainer.shap_values(X = self.X_scaled[row_ind:row_ind+1], n_samples = sampling_times)
        
        self.explain_instance = shap_value_single
        
        self.expected_value = shap_explainer.expected_value
        
        return self
    
    # function to generate list for top features for specified target
    def shap_top_features(self, target, num_features=50):
        
        target_index = self.full_targets.index(target)

        # Create table with shapley values for features
        df = pd.DataFrame({'Feature name':self.features,
                              'SHAPley value':self.explain_instance[target_index].reshape(-1,),
#                            'SHAPley value':self.explain_instance[:,:,target_index].reshape(-1,), # For newer versions
                           'Value':self.X_test[self.row_ind].reshape(-1,).round(decimals = 2)})
        
        # display missing value as NA 
        df['Value'] = df['Value'].replace(-1, 'NA')
        
        # aggregate the impact of features by category/group
        for variable in self.categorical_features:
            # filter out all dummy features that belong to the variable
            df_variable = df[df['Feature name'].isin(self.categorical_features[variable])].reset_index(drop=True)
            
            # sum up all the SHAPley values contributed by the variable's dummies
            sum_relative_imp = sum(df_variable['SHAPley value'])
            shapley_values = df_variable['SHAPley value'].values
            # get the dummy name dummy with value 1
            try:
                ind = df_variable[df_variable['Value']==1].index
                feature_category = df_variable.iloc[ind[0]]['Feature name'].rsplit("_")[1]
            except:
                feature_category = 'Unknown'
            
            # add the combined values into one entry in the importance df
            tmp = pd.DataFrame({'Feature name': variable, 
                        'SHAPley value': sum_relative_imp, 
                        'Value': feature_category,
                        'Indiv_SHAPley values': [shapley_values]
                               }, index=[0])
            df = pd.concat([df, tmp], axis=0, ignore_index=True)
            
            # drop the individual dummies from the importance df
            df = df[~df['Feature name'].isin(self.categorical_features[variable])]

        
        for variable in self.time_features:
            # filter out all related features that belong to the variable
            df_variable = df[df['Feature name'].isin(self.time_features[variable])].reset_index(drop=True)
            
            # sum up all the SHAPley values contributed by the variables
            sum_relative_imp = sum(df_variable['SHAPley value'])
            
            # get the feature values 
            values = [df_variable['Value'][df_variable['Feature name']==period+variable
                                          ].values[0] for period in ['YEARH_', 'YEAR1_', 'YEAR1H_', 'YEAR2_', 'YEAR2H_', 'YEAR3_']]  
            
            shapley_values = df_variable['SHAPley value'].values
            
            # add the combined values into one entry in the importance df
            tmp = pd.DataFrame({'Feature name': variable, 
                        'SHAPley value': sum_relative_imp, 
                        'Value': [values],
                        'Indiv_SHAPley values': [shapley_values]
                               }, index=[0])
            df = pd.concat([df, tmp], axis=0, ignore_index=True)
            
            # drop the individual dummies from the importance df
            df = df[~df['Feature name'].isin(self.time_features[variable])]

        
        # sort df based on importance 
        df = df.sort_values(by = ["SHAPley value"], ascending=False)[:num_features].reset_index(drop=True)

        return df
    
    def plot_shap_force(self, target, row_ind):
        target_index = self.full_targets.index(target)
    
        self.shap_explainer_generator(row_ind=row_ind)
        shap_df = self.shap_top_features(target=target, num_features=1000)
        shap_df['Abs_SHAP'] = np.abs(shap_df['SHAPley value'])
        shap_df = shap_df.sort_values('Abs_SHAP', ascending=False)
        shap_df['FeatureNum'] = [f"Feature {i+1}" for i in range(len(shap_df))]
        
        plt.figure(figsize=(30, 6))
        shap.plots.force(base_value=self.expected_value[target_index],
                    shap_values=np.array(shap_df['SHAPley value']),
                     feature_names=np.array(shap_df['FeatureNum']),
                     matplotlib=True,
                    show=False
                    )     

        return plt.gcf(), shap_df
    
    # create function to generate shap explainer for multiple samples
    # use for feature selection: generating "aggregated importance"
    def shap_explainer_generator_multiple(self, num_samples=50, sampling_times=500):
    
        num_samples = min(num_samples, len(self.X_test))      
        test_df = self.X_scaled[:num_samples]
    
        shap_explainer = shap.KernelExplainer(self.model, self.train_df, seed=random_state)
        
        shap_value_multiple = shap_explainer.shap_values(X = test_df, n_samples = sampling_times)
        
        self.explain_multiple = shap_value_multiple
        print("Generating SHAP explanation for {} samples and re-evaluating {} times. ".format(num_samples, sampling_times))
        
        return self
    
    # function to generate list for top features for specified target
    def shap_top_features_multiple(self, target, num_features=50):
        target_index = self.targets.index(target)
        
        df = pd.DataFrame(columns=['Feature name', 'SHAPley value'])
        
        df['Feature name'] = self.features
        df['SHAPley value'] = np.average(np.abs(self.explain_multiple[target_index]), axis=0)
        
        # Handle categorical features
        for variable in self.categorical_features:
            df_variable = df[df['Feature name'].isin(self.categorical_features[variable])].reset_index(drop=True)
            if not df_variable.empty:
                sum_relative_imp = sum(df_variable['SHAPley value'])
                df = pd.concat([df, pd.DataFrame({'Feature name': [variable],
                    'SHAPley value': [sum_relative_imp]})], ignore_index=True)
                df = df[~df['Feature name'].isin(self.categorical_features[variable])]

        # Handle time features
        for variable in self.time_features:
            df_variable = df[df['Feature name'].isin(self.time_features[variable])].reset_index(drop=True)
            if not df_variable.empty:
                sum_relative_imp = sum(df_variable['SHAPley value'])
                df = pd.concat([df, pd.DataFrame({'Feature name': [variable],
                    'SHAPley value': [sum_relative_imp]})], ignore_index=True)
                df = df[~df['Feature name'].isin(self.time_features[variable])]
        
        df = df.sort_values(by=["SHAPley value"], ascending=False)
        df = df[:num_features].reset_index(drop=True)
        return df
    
    def _aggregate_shap_values(self, shap_values, feature_values):
        """
        Helper function to aggregate SHAP values for categorical and time features
        """
        # Ensure proper shapes
        values = np.array(shap_values)
        if values.ndim == 1:
            values = values.reshape(1, -1)
        feature_values = np.array(feature_values)
        if feature_values.ndim == 1:
            feature_values = feature_values.reshape(1, -1)
            
        features = np.array(self.features)
        aggregated_values = []
        aggregated_features = []
        aggregated_feature_values = []
        
        # Track which features have been processed
        mask = np.ones(len(features), dtype=bool)
        
        # Handle categorical features
        for variable in self.categorical_features:
            indices = [i for i, f in enumerate(features) if f in self.categorical_features[variable]]
            if indices:
                mask[indices] = False
                sum_value = np.sum(values[:, indices], axis=1)
                aggregated_values.append(sum_value)
                aggregated_features.append(variable)
                # Get the active category value
                cat_values = feature_values[:, indices]
                active_cat_idx = np.argmax(cat_values, axis=1)
                aggregated_feature_values.append(cat_values[np.arange(len(cat_values)), active_cat_idx])
        
        # Handle time features
        for variable in self.time_features:
            indices = [i for i, f in enumerate(features) if f in self.time_features[variable]]
            if indices:
                mask[indices] = False
                sum_value = np.sum(values[:, indices], axis=1)
                aggregated_values.append(sum_value)
                aggregated_features.append(variable)
                aggregated_feature_values.append(np.mean(feature_values[:, indices], axis=1))
        
        # Add remaining features
        remaining_indices = np.where(mask)[0]
        for idx in remaining_indices:
            aggregated_values.append(values[:, idx])
            aggregated_features.append(features[idx])
            aggregated_feature_values.append(feature_values[:, idx])
        
        if not aggregated_values:  # Handle empty case
            return np.array([]), np.array([]), np.array([])
            
        return (np.stack(aggregated_values).T,
                np.array(aggregated_features),
                np.stack(aggregated_feature_values).T)

    def plot_shap_bar(self, target, max_display=20, exclude_features=None, save_path=None):
        """
        Create a SHAP bar plot showing mean absolute SHAP values for the specified target.
        Optionally save the plot as an image.
        """
        target_index = self.targets.index(target)
        shap_values = self.explain_multiple[target_index]

        # Aggregate SHAP values
        agg_values, agg_features, _ = self._aggregate_shap_values(
            shap_values, 
            self.X_scaled[:len(shap_values)]
        )

        # Exclude specified features
        if exclude_features:
            mask = ~np.isin(agg_features, exclude_features)
            agg_values = agg_values[:, mask]
            agg_features = agg_features[mask]

        plt.figure(figsize=(12, 6))  # Adjusted figure size to prevent x-axis cutoff
        shap.summary_plot(
            agg_values,
            feature_names=agg_features,
            plot_type="bar",
            max_display=max_display,
            show=False
        )
        plt.title(f'SHAP Feature Importance for {target}')
        plt.gca().xaxis.label.set_size(10)  # Adjust font size of x-axis label without changing the label text
        plt.tight_layout()

        # Save the plot if save_path is provided
        if save_path:
            plt.savefig(save_path, bbox_inches='tight')

        return plt.gcf()

    def plot_shap_beeswarm(self, target, max_display=20, exclude_features=None, save_path=None):
        """
        Create a SHAP beeswarm plot showing the distribution of SHAP values for the specified target.
        Optionally save the plot as an image.
        """
        target_index = self.targets.index(target)
        shap_values = self.explain_multiple[target_index]
        feature_values = self.X_scaled[:len(shap_values)]

        # Aggregate SHAP values
        agg_values, agg_features, agg_feature_values = self._aggregate_shap_values(
            shap_values, 
            feature_values
        )

        # Exclude specified features
        if exclude_features:
            mask = ~np.isin(agg_features, exclude_features)
            agg_values = agg_values[:, mask]
            agg_features = agg_features[mask]
            agg_feature_values = agg_feature_values[:, mask]

        plt.figure(figsize=(12, 8))  # Adjusted figure size to prevent x-axis cutoff
        shap.summary_plot(
            agg_values,
            agg_feature_values,
            feature_names=list(agg_features),
            max_display=max_display,
            show=False
        )
        plt.title(f'SHAP Feature Impact for {target}')
        plt.tight_layout()

        # Save the plot if save_path is provided
        if save_path:
            plt.savefig(save_path, bbox_inches='tight')

        return plt.gcf()